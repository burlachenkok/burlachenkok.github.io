---
layout: post
title: Faster rates for compressed federated learning with client-variance reduction
published: true
---

The new paper ["Faster rates for compressed federated learning with client-variance reduction"](https://arxiv.org/abs/2112.13097) has been out.

---

Our paper ["Faster rates for compressed federated learning with client-variance reduction"](https://arxiv.org/abs/2112.13097) has been out. I was glad to work with my peers [Haoyu Zhao](https://hyzhao.me/) from [Princeton University](https://www.princeton.edu/), [Zhize Li](https://zhizeli.github.io/), and prof. [Peter Richtarik](https://richtarik.org/) from [King Abdullah University of Science and Technology](https://cemse.kaust.edu.sa/).

We provide rigorous theory and a rich amount of practical experiments to highlight the benefits of our methods. In terms of practice, we are providing comparisons of several state-of-the-art optimization Federated Learning (FL) algorithms with theoretical and tunable parameters that control the behavior of optimization algorithms.

The provided experiments include non-convex binary classification with convex and nonconvex logistic regression and Image classification ResNet-18@CIFAR10.

Our [COFIG](https://arxiv.org/abs/2112.13097) algorithm has demonstrated in honest comparison excellent results.

The experimental part for that paper has been done in an advanced research simulator for Federated Learning called [FL_PyTorch](https://dl.acm.org/doi/10.1145/3488659.3493775).
