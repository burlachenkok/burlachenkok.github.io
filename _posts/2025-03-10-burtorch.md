---
layout: post
title: BurTorch (Backpropagation Ultrafast Runtime)
published: true
---

<center>
<img height="80px" src="https://burlachenkok.github.io/images/burtorch-logo-2.png"/>
</center>

The new paper [BurTorch: Revisiting Training from First Principles by Coupling Autodiff, Math Optimization, and Systems](https://arxiv.org/abs/2405.14852) presents the design choices behind one of the fastest and memory-efficient backpropagation implementations on CPU.

---

# Results

The new paper by [Konstantin Burlachenko](https://burlachenkok.github.io/) and [Peter Richt√°rik](https://richtarik.org/), present and compares [BurTorch](https://arxiv.org/abs/2405.14852) with well-known industry best-practice solutions used across different operation modes and programming language APIs. The list of frameworks includes the following:

* **PyTorch**: [https://pytorch.org/](https://pytorch.org/)
* **JAX**: [https://docs.jax.dev/en/latest/](https://docs.jax.dev/en/latest/)
* **TensorFlow**: [https://www.tensorflow.org/](https://www.tensorflow.org/)
* **Autograd**: [https://github.com/HIPS/autograd](https://github.com/HIPS/autograd)
* **Micrograd**: [https://x.com/karpathy/status/1803963383018066272](https://x.com/karpathy/status/1803963383018066272)
* **Apple MLX**: [https://github.com/ml-explore/mlx](https://github.com/ml-explore/mlx)

Experiments were conducted on physically distinct computational devices and across major desktop operating systems:

* [macOS Sonoma 14.5](https://developer.apple.com/documentation/macos-release-notes/macos-14_5-release-notes)
* [Ubuntu 20.04.6 LTS](https://releases.ubuntu.com/focal/)
* [Microsoft Windows 11 Home](https://www.microsoft.com/en-us/d/windows-11-home/dg7gmgf0krt0)

For small compute graphs, BurTorch outperforms best-practice solutions by up to **x2000** in runtime and reduces memory consumption by up to **x3500**. 

For a miniaturized [GPT-3 model (Brown et al., 2020)](https://arxiv.org/abs/2005.14165), BurTorch achieves up to a **x20** speedup and reduces memory up to **x80** compared to [PyTorch](https://proceedings.neurips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html) on CPU.

----

# Extra Links

- **The arXiv preprint**: [https://arxiv.org/abs/2410.08760](https://arxiv.org/abs/2410.08760)
- **Podcast** generated via [NotebookLM](https://notebooklm.google/) in an entertaining format: [(i) online](https://www.podbean.com/eas/pb-zs34b-16d2942) and [(ii) offline](https://burlachenkok.github.io/podcasts/butrch-generated-interview.mp3).

---

# Abstract

In this work, we introduce BurTorch, a compact high-performance framework designed to optimize Deep Learning (DL) training on single-node workstations 
through an exceptionally efficient CPU-based backpropagation ([Rumelhart et al., 1986](https://www.nature.com/articles/323533a0); [Linnainmaa, 1970](https://scholar.googleusercontent.com/scholar.bib?q=info:wRjDZKQ_NKYJ:scholar.google.com/&output=citation&scisdr=ClHdwmNeENKs6Xb1i_s:AFWwaeYAAAAAZ87zk_vuPijL7H0txyMVOwPA1wQ&scisig=AFWwaeYAAAAAZ87zk4D5Rjhb-wNl_c2IxQBTkcc&scisf=4&ct=citation&cd=-1&hl=ru)) implementation. Although modern DL frameworks rely on compiler-like optimizations internally, BurTorch takes a different path. It adopts a minimalist design and demonstrates that, 
in these circumstances, classical compiled programming languages can play a significant role in DL research. 
By eliminating the overhead of large frameworks and making efficient implementation choices, BurTorch achieves orders-of-magnitude improvements in performance and memory efficiency when 
computing the gradient of a function on a CPU. BurTorch features a compact codebase designed to achieve two key goals simultaneously. 
First, it provides a user experience similar to script-based programming environments. 
Second, it dramatically minimizes runtime overheads. In large DL frameworks, the primary source of memory overhead for relatively small computation graphs is due to feature-heavy implementations. 
We benchmarked BurTorch against widely used DL frameworks in their execution modes: 
[JAX (Bradbury et al., 2018)](https://github.com/jax-ml/jax), [PyTorch (Paszke et al., 2019)](https://proceedings.neurips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html), [TensorFlow (Abadi et al., 2016)](https://arxiv.org/abs/1605.08695);
and several standalone libraries: [Autograd (Maclaurin et al., 2015)](https://github.com/HIPS/autograd), [Micrograd (Karpathy, 2020)](https://github.com/karpathy/micrograd), [Apple MLX (Hannun et al., 2023)](https://github.com/ml-explore).
For small compute graphs, BurTorch outperforms best-practice solutions by up to x2000 in runtime and reduces memory consumption by up to x3500. For a miniaturized [GPT-3 model (Brown et al., 2020)](https://arxiv.org/abs/2005.14165), 
BurTorch achieves up to a x20 speedup and reduces memory up to x80 compared to [PyTorch](https://proceedings.neurips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html).

---

# Source Code

[BurTorch](https://arxiv.org/abs/2405.14852), like any high-performance framework with low-latency capabilities, carries the potential for misuse if it falls into the wrong hands. 
The authors require time to assess the risks before proceeding with the distribution of the source code in any form other than for the peer-review process.

<center>
<table style="text-align:center;">
<tr>
<td style="padding:30px;text-align:center;vertical-align:middle;"> <img height="115px" src="https://burlachenkok.github.io/materials/KAUST-logo.svg"/> </td>
</tr>
</table>
</center>
