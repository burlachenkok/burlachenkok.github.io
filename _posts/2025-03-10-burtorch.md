---
layout: post
title: BurTorch. Backpropagation Ultrafast Runtime with a Torch-Like API
published: true
---

The new paper [BurTorch: Revisiting Training from First Principles by Coupling Autodiff, Math Optimization, and Systems](https://arxiv.org/abs/2405.14852) presents the design choices behind one of the fastest and memory-efficient backpropagation implementations on CPU.

---

# Results

We demonstrate that, in specific scenarios, [BurTorch](https://arxiv.org/abs/2405.14852) dramatically improves real wall-clock gradient oracle computation time, reduces memory footprint, and executes the required computations with lower energy consumption from the supplied battery.

The paper compares [BurTorch](https://arxiv.org/abs/2405.14852) with well-known industry best-practice solutions used across different operation modes and programming language APIs. The list of frameworks includes the following:

* [PyTorch](https://proceedings.neurips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html)
* [JAX](https://github.com/jax-ml/jax)
* [TensorFlow](https://arxiv.org/abs/1605.08695)
* [Autograd](https://github.com/HIPS/autograd)
* [Micrograd](https://github.com/karpathy/micrograd)
* [Apple MLX](https://github.com/ml-explore)


The benchmarks included experiments on computation graphs of varying scales, from a few intermediate operations to comparisons with a miniaturized GPT-3-like model.
Experiments were conducted across physically distinct computational devices and operating systems:

* macOS
* Ubuntu (Linux)
* Windows OS

The arXiv link for the paper: [https://arxiv.org/abs/2405.14852](https://arxiv.org/abs/2405.14852).

----

# Abstract

We introduce BurTorch, a high-performance, compact framework designed to optimize machine learning (ML) training on 
single-node workstations through a highly optimized implementation of backpropagation ([Rumelhart et al., 1986](https://www.nature.com/articles/323533a0); [Linnainmaa, 1970](https://scholar.googleusercontent.com/scholar.bib?q=info:wRjDZKQ_NKYJ:scholar.google.com/&output=citation&scisdr=ClHdwmNeENKs6Xb1i_s:AFWwaeYAAAAAZ87zk_vuPijL7H0txyMVOwPA1wQ&scisig=AFWwaeYAAAAAZ87zk4D5Rjhb-wNl_c2IxQBTkcc&scisf=4&ct=citation&cd=-1&hl=ru)).
While modern ML frameworks often rely on compile-like optimization stages to improve wall-clock time internally, BurTorch adopts a different approach. 
It embraces a minimalistic design that highlights the importance of classical compile-based programming languages in ML research while delivering a
fast backpropagation implementation on CPUs. Due to its small code size, BurTorch provides a user experience
nearly indistinguishable from script-based programming environments. By eliminating the overhead of large
frameworks and making efficient implementation choices, BurTorch achieves significant improvements in both
performance and memory efficiency when computing the gradient of a function. In specific use cases, it outperforms best-practice
solutions in terms of memory consumption and runtime by significant margins. The comparison includes: (1)
existing Deep Learning frameworks, such as [JAX (Bradbury et al., 2018)](https://github.com/jax-ml/jax), [PyTorch (Paszke et al., 2019)](https://proceedings.neurips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html), and [TensorFlow (Abadi et al., 2016)](https://arxiv.org/abs/1605.08695); 
(2) standalone Automatic Differentiation libraries, such as [Autograd (Maclaurin et al., 2015)](https://github.com/HIPS/autograd), [Micrograd (Karpathy, 2020)](https://github.com/karpathy/micrograd), and [Apple MLX (Hannun et al., 2023)](https://github.com/ml-explore). BurTorch demonstrates that optimizing the end-to-end runtime for gradient computation, including memory and system-level efficiency, can be even more crucial than solely focusing on algorithmic optimizations inside backpropagation.


---

<center>
<table style="text-align:center;">
<tr>
<td style="padding:30px;text-align:center;vertical-align:middle;"> <img height="100px" src="https://burlachenkok.github.io/materials/KAUST-logo.svg"/> </td>
<td style="padding:30px;text-align:center;vertical-align:middle;"> <img height="100px" src="https://burlachenkok.github.io/images/burtorch-logo-1-med.png"/> </td>
</tr>
</table>
</center>
