---
layout: post
title: BurTorch. Backpropagation Ultrafast Runtime with a Torch-Like API
published: true
---

The new paper [BurTorch: Revisiting Training from First Principles by Coupling Autodiff, Math Optimization, and Systems](https://arxiv.org/abs/2405.14852) with one of the fastest backpropagation implementation in CPU.

---

Our paper [BurTorch: Revisiting Training from First Principles by Coupling Autodiff, Math Optimization, and Systems](https://arxiv.org/abs/2405.14852). 

# Results

We demonstrate that [BurTorch](https://arxiv.org/abs/2405.14852) is drammatically improve real wall-clock gradient oracles computation time, drammatically decrease memory foorptint, and in addition execute need computation with less drained energy from the supplied battety source.

The paper compares [BurTorch](https://arxiv.org/abs/2405.14852) with well-knonwn industry standarts like:

* [JAX](https://github.com/jax-ml/jax)
* [PyTorch](https://proceedings.neurips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html)
* [TensorFlow](https://arxiv.org/abs/1605.08695)
* [Autograd](https://github.com/HIPS/autograd)
* [Micrograd](https://github.com/karpathy/micrograd)
* [Apple MLX](https://github.com/ml-explore)

The benchmarks was carried in the following OS and entails experiments for computation graphs with 5 intermediates scalars to a miniaturized GPT-3-like model

* macOS
* Ubuntu(Linux)
* Windows OS

The arXiv link for the paper: [https://arxiv.org/abs/2405.14852](https://arxiv.org/abs/2405.14852).

----

# Abstract

We introduce BurTorch, a high-performance, compact framework designed to optimize machine learning (ML) training on 
single-node workstations through a highly optimized implementation of backpropagation ([Rumelhart et al., 1986](https://www.nature.com/articles/323533a0); [Linnainmaa, 1970](https://scholar.googleusercontent.com/scholar.bib?q=info:wRjDZKQ_NKYJ:scholar.google.com/&output=citation&scisdr=ClHdwmNeENKs6Xb1i_s:AFWwaeYAAAAAZ87zk_vuPijL7H0txyMVOwPA1wQ&scisig=AFWwaeYAAAAAZ87zk4D5Rjhb-wNl_c2IxQBTkcc&scisf=4&ct=citation&cd=-1&hl=ru)).
While modern ML frameworks often rely on compile-like optimization stages to improve wall-clock time internally, BurTorch adopts a different approach. 
It embraces a minimalistic design that highlights the importance of classical compile-based programming languages in ML research while delivering a
fast backpropagation implementation on CPUs. Due to its small code size, BurTorch provides a user experience
nearly indistinguishable from script-based programming environments. By eliminating the overhead of large
frameworks and making efficient implementation choices, BurTorch achieves significant improvements in both
performance and memory efficiency when computing gradient of function. In specific use cases, it outperforms best-practice
solutions in terms of memory consumption and runtime by significant margins. The comparison includes: (1)
existing Deep Learning frameworks, such as [JAX (Bradbury et al., 2018)](https://github.com/jax-ml/jax), [PyTorch (Paszke et al., 2019)](https://proceedings.neurips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html), and [TensorFlow (Abadi et al., 2016)](https://arxiv.org/abs/1605.08695); 
(2) standalone Automatic Differentiation libraries, such as [Autograd (Maclaurin et al., 2015)](https://github.com/HIPS/autograd), [Micrograd (Karpathy, 2020)](https://github.com/karpathy/micrograd), and [Apple MLX (Hannun et al., 2023)](https://github.com/ml-explore). BurTorch demonstrates that optimizing the end-to-end runtime for gradient computation, including memory and system-level efficiency, can be even more crucial than solely focusing on algorithmic optimizations inside backpropagation.

# Source Code

The source code is not publicly available yet.

---

<table style="text-align:center;">
<tr>
<td style="padding:15px;text-align:center;vertical-align:middle;"> <img height="95px" src="https://burlachenkok.github.io/materials/KAUST-logo.svg"/> </td> 
<td style="padding:15px;text-align:center;vertical-align:middle;"> <img height="95px" src="https://burlachenkok.github.io/images/burtorch-logo-1.png"/> </td>
</tr>
</table>
