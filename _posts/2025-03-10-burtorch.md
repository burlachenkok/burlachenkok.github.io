---
layout: post
title: BurTorch (Backpropagation Ultrafast Runtime)
published: true
---

The new paper [BurTorch: Revisiting Training from First Principles by Coupling Autodiff, Math Optimization, and Systems](https://arxiv.org/abs/2405.14852) presents the design choices behind one of the fastest and memory-efficient backpropagation implementations on CPU.

---

# Results

We demonstrate that, in specific scenarios, [BurTorch](https://arxiv.org/abs/2405.14852) dramatically improves real wall-clock gradient oracle computation time, reduces memory footprint, and executes the required computations with lower energy consumption from the supplied battery.

The paper compares [BurTorch (Konstantin Burlachenko and Peter Richt√°rik)](https://arxiv.org/abs/2405.14852) with well-known industry best-practice solutions used across different operation modes and programming language APIs. The list of frameworks includes the following:

* **PyTorch**: [https://pytorch.org/](https://pytorch.org/)
* **JAX**: [https://docs.jax.dev/en/latest/](https://docs.jax.dev/en/latest/)
* **TensorFlow**: [https://www.tensorflow.org/](https://www.tensorflow.org/)
* **Autograd**: [https://github.com/HIPS/autograd](https://github.com/HIPS/autograd)
* **Micrograd**: [https://x.com/karpathy/status/1803963383018066272?s=46&t=eeDtc-9V3RFJFmPZy2M8RA](https://x.com/karpathy/status/1803963383018066272?s=46&t=eeDtc-9V3RFJFmPZy2M8RA), [https://github.com/karpathy/micrograd](https://github.com/karpathy/micrograd)
* **Apple MLX**: [https://github.com/ml-explore/mlx](https://github.com/ml-explore/mlx)

The benchmarks included experiments on computation graphs of varying scales, from a few intermediate operations to comparisons with a miniaturized GPT-3-like model.
Experiments were conducted across physically distinct computational devices and operating systems:

* [macOS Sonoma 14.5](https://developer.apple.com/documentation/macos-release-notes/macos-14_5-release-notes)
* [Ubuntu 20.04.6 LTS](https://releases.ubuntu.com/focal/)
* [Microsoft Windows 11 Home](https://www.microsoft.com/en-us/d/windows-11-home/dg7gmgf0krt0)

The arXiv link for the paper: [https://arxiv.org/abs/2405.14852](https://arxiv.org/abs/2405.14852).

----

# Extra Links

- **The arXiv preprint**: [https://arxiv.org/abs/2410.08760](https://arxiv.org/abs/2410.08760)
- **Podcast** generated via [NotebookLM](https://notebooklm.google/) in an entertaining format: [(i) online](https://www.podbean.com/eas/pb-zs34b-16d2942) and [(ii) offline](https://burlachenkok.github.io/podcasts/butrch-generated-interview.mp3).

---

# Abstract

In this work, we introduce BurTorch, a compact high-performance framework designed to optimize Deep Learning (DL) training on single-node workstations 
through an exceptionally efficient CPU-based backpropagation ([Rumelhart et al., 1986](https://www.nature.com/articles/323533a0); [Linnainmaa, 1970](https://scholar.googleusercontent.com/scholar.bib?q=info:wRjDZKQ_NKYJ:scholar.google.com/&output=citation&scisdr=ClHdwmNeENKs6Xb1i_s:AFWwaeYAAAAAZ87zk_vuPijL7H0txyMVOwPA1wQ&scisig=AFWwaeYAAAAAZ87zk4D5Rjhb-wNl_c2IxQBTkcc&scisf=4&ct=citation&cd=-1&hl=ru)) implementation. Although modern DL frameworks rely on compiler-like optimizations internally, BurTorch takes a different path. It adopts a minimalist design and demonstrates that, 
in these circumstances, classical compiled programming languages can play a significant role in DL research. 
By eliminating the overhead of large frameworks and making efficient implementation choices, BurTorch achieves orders-of-magnitude improvements in performance and memory efficiency when 
computing the gradient of a function on a CPU. BurTorch features a compact codebase designed to achieve two key goals simultaneously. 
First, it provides a user experience similar to script-based programming environments. 
Second, it dramatically minimizes runtime overheads. In large DL frameworks, the primary source of memory overhead for relatively small computation graphs is due to feature-heavy implementations. 
We benchmarked BurTorch against widely used DL frameworks in their execution modes: 
[JAX (Bradbury et al., 2018)](https://github.com/jax-ml/jax), [PyTorch (Paszke et al., 2019)](https://proceedings.neurips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html), [TensorFlow (Abadi et al., 2016)](https://arxiv.org/abs/1605.08695);
and several standalone libraries: [Autograd (Maclaurin et al., 2015)](https://github.com/HIPS/autograd), [Micrograd (Karpathy, 2020)](https://github.com/karpathy/micrograd), [Apple MLX (Hannun et al., 2023)](https://github.com/ml-explore).
For small compute graphs, BurTorch outperforms best-practice solutions by up to x2000 in runtime and reduces memory consumption by up to x3500. For a miniaturized [GPT-3 model (Brown et al., 2020)](https://arxiv.org/abs/2005.14165), 
BurTorch achieves up to a x20 speedup and reduces memory up to x80 compared to [PyTorch](https://proceedings.neurips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html).

---

# Source Code

BurTorch, like any high-performance framework with low-latency capabilities, holds the potential for misuse in non-beneficial ways, as with any advanced technology. 

The authors acknowledge their responsibility to remain mindful of this before proceeding with any form of source code distribution.

---

<center>
<table style="text-align:center;">
<tr>
<td style="padding:30px;text-align:center;vertical-align:middle;"> <img height="150px" src="https://burlachenkok.github.io/materials/KAUST-logo.svg"/> </td>
<td style="padding:30px;text-align:center;vertical-align:middle;"> <img height="256px" src="https://burlachenkok.github.io/images/burtorch-logo-1-med.png"/> </td>
</tr>
</table>
</center>
